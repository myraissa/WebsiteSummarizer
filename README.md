# Local Ollama Website Summarizer

This project demonstrates a website summarizer using a **local LLaMA model** via Ollama and a simple **Gradio interface**.

---

## Features

- Extracts the main text content from any URL.
- Removes irrelevant elements (scripts, images, styles, inputs).
- Generates a short, structured summary in **Markdown**.
- Uses a **local LLaMA model** (llama3.2) for fast offline inference.
- Optional streaming (future improvement).

---

## Why LLaMA?

- **High-quality text generation:** Excellent for summarization and NLP tasks.
- **Lightweight & local:** Can run on personal hardware without sending data to the cloud.
- **Customizable:** Works with Ollama for local inference pipelines.
- **Privacy:** All website data stays on your local machine.

---
