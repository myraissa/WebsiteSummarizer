# ğŸ¦™ Local Ollama Website Summarizer

This project demonstrates a **local website summarizer** powered by **Ollama** using a **LLaMA model (llama3.2)** and a simple **Gradio web interface**.

---

## ğŸš€ Features

- ğŸ§  **Content Extraction** â€“ Fetches and cleans the main text content from any URL.  
- ğŸ§¹ **Noise Removal** â€“ Automatically removes unnecessary elements (scripts, images, styles, inputs).  
- ğŸ“ **Structured Summaries** â€“ Produces short, readable summaries in **Markdown** format.  
- âš™ï¸ **Offline & Local** â€“ Runs entirely on your machine using the local **LLaMA model** for privacy and speed.  
- ğŸ”„ **Future Improvements** â€“ Planned support for real-time **streaming summaries**.

---

## ğŸ§© Project Overview

This tool is ideal for quickly understanding web articles, documentation, or blogs without needing online AI APIs.  
Simply enter a URL, and the app fetches, cleans, and summarizes the main content using your local Ollama setup.

---

## ğŸ–¥ï¸ Interface Preview

### 1ï¸âƒ£ Empty Interface (Before Input)
When you first open the app, you'll see a simple Gradio interface with an input box where you can paste a URL.

![Empty Interface](images/empty.png)


---

### 2ï¸âƒ£ With Summarization (After Processing)
After submitting a URL, the app fetches the content, processes it, and displays a concise summary generated by the **LLaMA model**.

![Summarized Interface](images/res.png)


---

## âš¡ Quick Start

1. **Install Ollama** (if not already):  
   ğŸ‘‰ [https://ollama.ai/download](https://ollama.ai/download)

2. **Run your LLaMA model locally:**  
   ```bash
   ollama run llama3.2
